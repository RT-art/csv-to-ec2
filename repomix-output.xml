This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.gitignore
manage.sh
README.md
sample.csv
src/app.py
template.yaml
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".gitignore">
# SAM build artifacts

.aws-sam/

# Local configuration file

.sam_outputs
</file>

<file path="manage.sh">
#!/bin/bash

# A script to manage the deployment and usage of the csv-to-ec2 SAM application.

# Stop script on any error
set -e

# --- Configuration ---
# File to store the outputs from the SAM deployment, like the S3 bucket name.
CONFIG_FILE=".sam_outputs"
DEFAULT_CSV="sample.csv"
STACK_NAME="csv-to-ec2-stack" # You can change this default stack name
REGION=$(aws configure get region)
[ -z "$REGION" ] && REGION="ap-northeast-1" # Default to Tokyo if not set

# --- Helper Functions ---
# Print usage instructions
usage() {
    echo "Usage: $0 {deploy|upload|delete}"
    echo
    echo "Commands:"
    echo "  deploy   : Builds and deploys the AWS SAM stack. Creates network resources, S3 bucket, and Lambda."
    echo "  upload   : Uploads a CSV file to the S3 bucket to trigger instance creation. Uses '$DEFAULT_CSV' by default."
    echo "  delete   : Deletes the entire AWS SAM stack and all created resources."
    echo
}

# --- Main Functions ---

# Build and Deploy the SAM stack
deploy_stack() {
    echo ">>> Building the SAM application..."
    sam build

    echo ">>> Deploying the stack '$STACK_NAME' to region '$REGION'..."
    # Using --guided for the first time is good, but subsequent deploys can use samconfig.toml
    sam deploy --stack-name "$STACK_NAME" --region "$REGION" --capabilities CAPABILITY_IAM --resolve-s3 --confirm-changeset

    echo ">>> Deployment successful. Fetching S3 bucket name..."
    
    # Get the S3 bucket name from the CloudFormation stack outputs
    BUCKET_NAME=$(aws cloudformation describe-stacks \
        --stack-name "$STACK_NAME" \
        --query "Stacks[0].Outputs[?OutputKey=='S3BucketName'].OutputValue" \
        --output text \
        --region "$REGION")

    if [ -z "$BUCKET_NAME" ]; then
        echo "Error: Could not retrieve S3 bucket name from stack outputs."
        echo "Please check the AWS Management Console for the stack status."
        exit 1
    fi

    # Save the bucket name to the config file for the 'upload' command
    echo "S3_BUCKET_NAME=$BUCKET_NAME" > "$CONFIG_FILE"
    
    echo "✔ Setup complete."
    echo "  S3 Bucket '$BUCKET_NAME' is ready."
    echo "  You can now use './manage.sh upload' to create EC2 instances."
}

# Upload a CSV file to the S3 bucket
# Upload a CSV file to the S3 bucket
upload_csv() {
    if [ ! -f "$CONFIG_FILE" ]; then
        echo "Error: Configuration file '$CONFIG_FILE' not found."
        echo "Please run './manage.sh deploy' first to set up the environment."
        exit 1
    fi

    # Load the bucket name from the config file
    source "$CONFIG_FILE"

    if [ -z "$S3_BUCKET_NAME" ]; then
        echo "Error: S3_BUCKET_NAME not found in '$CONFIG_FILE'."
        exit 1
    fi

    # --- vvv ここから変更 vvv ---

    # Determine the CSV file to upload.
    # If an argument is given, use it. Otherwise, find a unique .csv file.
    if [ -n "$1" ]; then
        CSV_FILE="$1"
    else
        # Find .csv files in the current directory (excluding subdirectories)
        CSV_FILES=$(find . -maxdepth 1 -type f -name "*.csv")
        # Count the number of found files (trims whitespace for accuracy)
        NUM_CSV_FILES=$(echo "$CSV_FILES" | sed '/^$/d' | wc -l | tr -d ' ')

        if [ "$NUM_CSV_FILES" -eq 1 ]; then
            CSV_FILE=$(echo "$CSV_FILES")
            echo ">>> Auto-detected CSV file: '$CSV_FILE'"
        elif [ "$NUM_CSV_FILES" -eq 0 ]; then
            echo "Error: No CSV file found in the current directory."
            echo "Please place a single .csv file here or specify the file path:"
            echo "  ./manage.sh upload <your-file.csv>"
            exit 1
        else
            echo "Error: Multiple CSV files found. Please specify which one to upload:"
            echo "$CSV_FILES"
            echo "Usage: ./manage.sh upload <file-to-upload.csv>"
            exit 1
        fi
    fi

    # --- ^^^ ここまで変更 ^^^ ---

    if [ ! -f "$CSV_FILE" ]; then
        echo "Error: CSV file '$CSV_FILE' not found."
        exit 1
    fi

    echo ">>> Uploading '$CSV_FILE' to bucket '$S3_BUCKET_NAME'..."
    aws s3 cp "$CSV_FILE" "s3://$S3_BUCKET_NAME/"

    echo "✔ File uploaded successfully. EC2 instance creation has been triggered."
}

# Delete the SAM stack
delete_stack() {
    echo ">>> Deleting the SAM stack '$STACK_NAME' from region '$REGION'..."
    echo "!!! This will remove all AWS resources created by this script. !!!"
    
    sam delete --stack-name "$STACK_NAME" --region "$REGION" --no-prompts

    # Clean up local config file
    if [ -f "$CONFIG_FILE" ]; then
        rm "$CONFIG_FILE"
    fi

    echo "✔ Stack deleted successfully."
}


# --- Script Entrypoint ---
# Check for required tools
if ! command -v sam &> /dev/null || ! command -v aws &> /dev/null; then
    echo "Error: 'sam' and 'aws' CLI tools are required."
    echo "Please install them and configure your AWS credentials."
    exit 1
fi


# Main command router
case "$1" in
    deploy)
        deploy_stack
        ;;
    upload)
        # Pass the second argument (the filename) to the upload function
        upload_csv "$2"
        ;;
    delete)
        delete_stack
        ;;
    *)
        usage
        exit 1
        ;;
esac

exit 0
</file>

<file path="src/app.py">
import boto3
import csv
import os
import urllib.parse

s3 = boto3.client('s3')
ec2 = boto3.client('ec2')

SSM_INSTANCE_PROFILE_ARN = os.environ.get('SSM_INSTANCE_PROFILE_ARN')

def lambda_handler(event, context):
    """
    S3へのCSVファイルアップロードをトリガーに実行されるメイン関数。
    """
    # イベント情報からバケット名とファイル名（キー）を取得
    bucket = event['Records'][0]['s3']['bucket']['name']
    key = urllib.parse.unquote_plus(event['Records'][0]['s3']['object']['key'], encoding='utf-8')
    
    print(f"処理開始: s3://{bucket}/{key}")

    try:
        subnet_id = os.environ['TARGET_SUBNET_ID']
    except KeyError:
        print("Fatal error: TARGET_SUBNET_ID environment variable is not set.")
        raise

    try:
        response = s3.get_object(Bucket=bucket, Key=key)
        lines = response['Body'].read().decode('utf-8').splitlines()
        reader = csv.DictReader(lines)

        for row in reader:
            try:
                ami_id = row.get('ami_id')
                instance_type = row.get('instance_type')

                if not all([ami_id, instance_type]):
                    print(f"Skipping row due to missing required fields: {row}")
                    continue

                run_instances_params = {
                    'ImageId': ami_id,
                    'InstanceType': instance_type,
                    'SubnetId': subnet_id,
                    'MinCount': 1,
                    'MaxCount': 1,
                    'TagSpecifications': [{
                        'ResourceType': 'instance',
                        'Tags': [
                            {'Key': 'Name', 'Value': f'auto-created-from-{os.path.basename(key)}'},
                            {'Key': 'SourceFile', 'Value': f's3://{bucket}/{key}'}
                        ]
                    }]
                }

                enable_ssm = row.get('enable_ssm', 'OFF').upper() == 'ON'
                
                if enable_ssm and SSM_INSTANCE_PROFILE_ARN:
                    run_instances_params['IamInstanceProfile'] = {
                        'Arn': SSM_INSTANCE_PROFILE_ARN
                    }
                    print(f"Creating EC2 instance with SSM enabled: AMI={ami_id}, Type={instance_type}")
                else:
                    print(f"Creating EC2 instance: AMI={ami_id}, Type={instance_type}")
                
                instance_response = ec2.run_instances(**run_instances_params)
                
                instance_id = instance_response['Instances'][0]['InstanceId']
                print(f"Instance creation successful: {instance_id}")

            except Exception as e:
                print(f"Error processing row, skipping: {row}, Error: {e}")

    except Exception as e:
        print(f"Fatal error occurred: {e}")
        raise e

    print(f"処理完了: s3://{bucket}/{key}")
    return {'status': 'success'}
</file>

<file path="sample.csv">
ami_id,instance_type,enable_ssm
ami-0c55b159cbfafe1f0,t2.micro,ON
ami-0c55b159cbfafe1f0,t3.small,OFF
ami-0c55b159cbfafe1f0,t2.nano,
</file>

<file path="template.yaml">
AWSTemplateFormatVersion: '2010-09-09'
Transform: AWS::Serverless-2016-10-31
Description: >-
  Stack to automatically create EC2 instances from CSV files uploaded to S3.
  Creates VPC, Subnet, S3 bucket, and Lambda function.

Resources:
  # ------------------------------------------------------------
  #  Network Resources
  # ------------------------------------------------------------
  VPC:
    Type: AWS::EC2::VPC
    Properties:
      CidrBlock: 10.0.0.0/16
      EnableDnsSupport: true
      EnableDnsHostnames: true
      Tags:
        - Key: Name
          Value: !Sub "${AWS::StackName}-VPC"

  InternetGateway:
    Type: AWS::EC2::InternetGateway
    Properties:
      Tags:
        - Key: Name
          Value: !Sub "${AWS::StackName}-IGW"

  VPCGatewayAttachment:
    Type: AWS::EC2::VPCGatewayAttachment
    Properties:
      VpcId: !Ref VPC
      InternetGatewayId: !Ref InternetGateway

  PublicSubnet:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref VPC
      CidrBlock: 10.0.1.0/24
      MapPublicIpOnLaunch: true 
      Tags:
        - Key: Name
          Value: !Sub "${AWS::StackName}-PublicSubnet"

  RouteTable:
    Type: AWS::EC2::RouteTable
    Properties:
      VpcId: !Ref VPC
      Tags:
        - Key: Name
          Value: !Sub "${AWS::StackName}-RouteTable"

  DefaultRoute:
    Type: AWS::EC2::Route
    DependsOn: VPCGatewayAttachment
    Properties:
      RouteTableId: !Ref RouteTable
      DestinationCidrBlock: 0.0.0.0/0
      GatewayId: !Ref InternetGateway

  SubnetRouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref PublicSubnet
      RouteTableId: !Ref RouteTable

  # ------------------------------------------------------------
  #  IAM Role for EC2
  # ------------------------------------------------------------
  EC2InstanceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service: "ec2.amazonaws.com"
            Action: "sts:AssumeRole"
      ManagedPolicyArns:
        - "arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore"
      Tags:
        - Key: Name
          Value: !Sub "${AWS::StackName}-EC2InstanceRole"

  EC2InstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Roles:
        - !Ref EC2InstanceRole

  # ------------------------------------------------------------
  #  S3 Bucket
  # ------------------------------------------------------------
  CsvFileBucket:
    Type: AWS::S3::Bucket
    Properties:
      # Bucket name must be globally unique, so includes Account ID and Region
      BucketName: !Sub "csv-to-ec2-${AWS::AccountId}-${AWS::Region}"
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true

  # ------------------------------------------------------------
  #  Lambda Function & IAM Role
  # ------------------------------------------------------------
  CsvToEc2Function:
    Type: AWS::Serverless::Function
    Properties:
      FunctionName: !Sub "${AWS::StackName}-CsvToEc2Function"
      Handler: app.lambda_handler
      Runtime: python3.13
      CodeUri: src/
      Timeout: 60
      Environment:
        Variables:
          TARGET_SUBNET_ID: !Ref PublicSubnet
          SSM_INSTANCE_PROFILE_ARN: !GetAtt EC2InstanceProfile.Arn
      Policies:
        # 1. Permission to read CSV files from S3 bucket
        - S3ReadPolicy:
            BucketName: !GetAtt CsvFileBucket.Name
        # 2. Permission to create and tag EC2 instances
        - Statement:
          - Effect: Allow
            Action:
              - ec2:RunInstances
              - ec2:CreateTags
            Resource: "*" # In production, specify resources more strictly
          - Effect: Allow
            Action: iam:PassRole
            Resource: !GetAtt EC2InstanceRole.Arn
      # Lambda trigger configuration
      Events:
        S3CsvUpload:
          Type: S3
          Properties:
            Bucket: !Ref CsvFileBucket
            Events: s3:ObjectCreated:*
            Filter:
              S3Key:
                Rules:
                  - Name: suffix
                    Value: .csv

Outputs:
  S3BucketName:
    Value: !Ref CsvFileBucket
  VpcId:
    Value: !Ref VPC
  PublicSubnetId:
    Value: !Ref PublicSubnet
</file>

<file path="README.md">
# サーバーレスEC2プロビジョニング from CSV ✨

CSVファイルに基づき、EC2インスタンスを自動で作成するAWS SAMアプリケーションです。

## 🏛️ アーキテクチャ

![アーキテクチャ図](images/アーキテクチャ図.png)

-----

## 🚀 使い方

### **前提条件**

  * [AWS CLI](https://aws.amazon.com/cli/) と [AWS SAM CLI](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-cli-install.html) がインストールされていること。
  * AWS認証情報が設定済みであること (`aws configure`)。

-----

### **Step 1: デプロイ (初回のみ)**

最初に、AWS環境に必要なリソース一式（VPC, S3バケット, Lambda関数など）をデプロイします。この操作は一度だけ行います。

1.  **スクリプトに実行権限を付与します。**

    ```bash
    chmod +x manage.sh
    ```

2.  **デプロイコマンドを実行します。**

    ```bash
    ./manage.sh deploy
    ```

    CloudFormationスタックの作成が完了すれば、環境の準備は完了です。

-----

### **Step 2: EC2インスタンスの作成**

1.  **CSVファイルを用意します。**
    `sample.csv` を参考に、作成したいEC2インスタンスの情報を記述します。

      * `ami_id` と `instance_type` の列は**必須**です。
      * `enable_ssm` 列は任意です。`ON`と指定すると、そのインスタンスでSSMセッションマネージャーが利用可能になります。

2.  **CSVファイルをアップロードしてインスタンスを作成します。**
    `upload` コマンドを実行すると、Lambda関数がトリガーされ、CSVの内容に基づいてEC2インスタンスが自動で作成されます。

      * **【推奨】CSVファイルが1つだけの場合**
        カレントディレクトリにCSVファイルが1つだけ存在する場合、ファイル名を指定しなくても自動で検知してアップロードします。

        ```bash
        # カレントディレクトリにある唯一的の.csvファイルをアップロード
        ./manage.sh upload
        ```

      * **特定のファイルを指定する場合**
        複数のCSVファイルがある場合や、特定のファイルを指定したい場合は、ファイル名を引数として渡します。

        ```bash
        # my_instances.csv を指定してアップロード
        ./manage.sh upload my_instances.csv
        ```

-----

### **Step 3: クリーンアップ 🗑️**

このアプリケーションで作成したすべてのAWSリソースを削除するには、以下のコマンドを実行します。

```bash
./manage.sh delete
```

VPC、S3バケット、Lambda関数、IAMロールなど、関連リソースが一括で削除されます。
</file>

</files>
