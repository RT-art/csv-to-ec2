This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.aws-sam/build.toml
.aws-sam/build/CsvToEc2Function/app.py
.aws-sam/build/template.yaml
README.md
sample.csv
src/app.py
template.yaml
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".aws-sam/build/CsvToEc2Function/app.py">
# src/app.py
import boto3
import csv
import os
import urllib.parse

s3 = boto3.client('s3')
ec2 = boto3.client('ec2')

def lambda_handler(event, context):
    bucket = event['Records'][0]['s3']['bucket']['name']
    key = urllib.parse.unquote_plus(event['Records'][0]['s3']['object']['key'], encoding='utf-8')
    
    print(f"Processing file: s3://{bucket}/{key}")

    try:
        response = s3.get_object(Bucket=bucket, Key=key)
        lines = response['Body'].read().decode('utf-8').splitlines()
        reader = csv.DictReader(lines)

        for row in reader:
            try:
                subnet_id = row.get('subnet_id')
                ami_id = row.get('ami_id')
                instance_type = row.get('instance_type')

                if not all([subnet_id, ami_id, instance_type]):
                    print(f"Skipping row due to missing required fields: {row}")
                    continue
                
                print(f"Creating EC2 instance with AMI: {ami_id}, Type: {instance_type} in Subnet: {subnet_id}")

                instance_response = ec2.run_instances(
                    ImageId=ami_id,
                    InstanceType=instance_type,
                    SubnetId=subnet_id,
                    MinCount=1,
                    MaxCount=1,
                    TagSpecifications=[{
                        'ResourceType': 'instance',
                        'Tags': [
                            {'Key': 'Name', 'Value': f'Created from {os.path.basename(key)}'},
                            {'Key': 'SourceFile', 'Value': f's3://{bucket}/{key}'}
                        ]
                    }]
                )
                
                instance_id = instance_response['Instances'][0]['InstanceId']
                print(f"Successfully created instance: {instance_id}")

            except Exception as e:
                print(f"Error processing row: {row}. Error: {str(e)}")

    except Exception as e:
        print(f"Error getting file from S3. Error: {str(e)}")
        raise e
</file>

<file path=".aws-sam/build.toml">
# This file is auto generated by SAM CLI build command

[function_build_definitions.588ab450-6ac9-447f-bce9-cc1a830f7cb5]
codeuri = "C:\\Work\\github\\csv-to-ec2\\src"
runtime = "python3.13"
architecture = "x86_64"
handler = "app.lambda_handler"
manifest_hash = ""
packagetype = "Zip"
functions = ["CsvToEc2Function"]

[layer_build_definitions]
</file>

<file path="src/app.py">
import boto3
import csv
import os
import urllib.parse

# boto3クライアントを初期化
s3 = boto3.client('s3')
ec2 = boto3.client('ec2')

def lambda_handler(event, context):
    """
    S3へのCSVファイルアップロードをトリガーに実行されるメイン関数。
    """
    # イベント情報からバケット名とファイル名（キー）を取得
    bucket = event['Records'][0]['s3']['bucket']['name']
    key = urllib.parse.unquote_plus(event['Records'][0]['s3']['object']['key'], encoding='utf-8')
    
    print(f"処理開始: s3://{bucket}/{key}")

    try:
        # S3からアップロードされたファイルを取得
        response = s3.get_object(Bucket=bucket, Key=key)
        
        # ファイルの内容を読み込み、UTF-8でデコードして行ごとに分割
        lines = response['Body'].read().decode('utf-8').splitlines()
        
        # CSVのヘッダーをキーとする辞書として各行を読み込む
        reader = csv.DictReader(lines)

        # CSVの各行をループ処理
        for row in reader:
            try:
                # 必要なパラメータをCSVから取得
                ami_id = row.get('ami_id')
                instance_type = row.get('instance_type')
                subnet_id = row.get('subnet_id')

                # 必須項目が欠けている行はスキップ
                if not all([ami_id, instance_type, subnet_id]):
                    print(f"必須項目が不足しているためスキップします: {row}")
                    continue

                print(f"EC2インスタンスを作成します: AMI={ami_id}, Type={instance_type}, Subnet={subnet_id}")

                # EC2インスタンスを作成
                instance_response = ec2.run_instances(
                    ImageId=ami_id,
                    InstanceType=instance_type,
                    SubnetId=subnet_id,
                    MinCount=1,
                    MaxCount=1,
                    # インスタンスに自動でタグを付与する設定
                    TagSpecifications=[{
                        'ResourceType': 'instance',
                        'Tags': [
                            {'Key': 'Name', 'Value': f'auto-created-from-{os.path.basename(key)}'},
                            {'Key': 'SourceFile', 'Value': f's3://{bucket}/{key}'}
                        ]
                    }]
                )
                
                instance_id = instance_response['Instances'][0]['InstanceId']
                print(f"インスタンス作成成功: {instance_id}")

            except Exception as e:
                # 行単位でのエラー処理
                print(f"エラーが発生したため、この行の処理を中断します: {row}, エラー: {e}")

    except Exception as e:
        # ファイル取得や全体的なエラー処理
        print(f"致命的なエラーが発生しました: {e}")
        raise e

    print(f"処理完了: s3://{bucket}/{key}")
    return {'status': 'success'}
</file>

<file path=".aws-sam/build/template.yaml">
AWSTemplateFormatVersion: '2010-09-09'
Transform: AWS::Serverless-2016-10-31
Description: A simple and refined stack to create an EC2 instance from a CSV file
  in S3 using Lambda.
Resources:
  S3Bucket:
    Type: AWS::S3::Bucket
    DependsOn: S3InvokePermission
    Properties:
      BucketName:
        Fn::Sub: csv-ec2-creator-${AWS::AccountId}-${AWS::Region}
      NotificationConfiguration:
        LambdaConfigurations:
        - Event: s3:ObjectCreated:*
          Function:
            Fn::GetAtt:
            - CsvToEc2Function
            - Arn
          Filter:
            S3Key:
              Rules:
              - Name: suffix
                Value: .csv
  CsvToEc2Function:
    Type: AWS::Serverless::Function
    Properties:
      FunctionName:
        Fn::Sub: ${AWS::StackName}-CsvToEc2Function
      Handler: app.lambda_handler
      Runtime: python3.13
      Timeout: 60
      CodeUri: CsvToEc2Function
      Policies:
      - Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Action: s3:GetObject
          Resource:
            Fn::Sub: arn:aws:s3:::csv-ec2-creator-${AWS::AccountId}-${AWS::Region}/*
        - Effect: Allow
          Action: ec2:RunInstances
          Resource:
          - Fn::Sub: arn:aws:ec2:${AWS::Region}:${AWS::AccountId}:instance/*
          - Fn::Sub: arn:aws:ec2:${AWS::Region}:${AWS::AccountId}:volume/*
          - Fn::Sub: arn:aws:ec2:${AWS::Region}:${AWS::AccountId}:network-interface/*
          - Fn::Sub: arn:aws:ec2:${AWS::Region}:${AWS::AccountId}:security-group/*
          - Fn::Sub: arn:aws:ec2:${AWS::Region}:${AWS::AccountId}:subnet/*
          - Fn::Sub: arn:aws:ec2:${AWS::Region}::image/*
        - Effect: Allow
          Action: ec2:CreateTags
          Resource:
            Fn::Sub: arn:aws:ec2:${AWS::Region}:${AWS::AccountId}:instance/*
          Condition:
            StringEquals:
              ec2:CreateAction: RunInstances
    Metadata:
      SamResourceId: CsvToEc2Function
  S3InvokePermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName:
        Fn::GetAtt:
        - CsvToEc2Function
        - Arn
      Action: lambda:InvokeFunction
      Principal: s3.amazonaws.com
      SourceArn:
        Fn::Sub: arn:aws:s3:::csv-ec2-creator-${AWS::AccountId}-${AWS::Region}
Outputs:
  S3BucketName:
    Description: Bucket to upload CSV files.
    Value:
      Ref: S3Bucket
</file>

<file path="sample.csv">
ami_id,instance_type,subnet_id
ami-0c55b159cbfafe1f0,t2.micro,subnet-xxxxxxxxxxxxxxxxx
ami-0c55b159cbfafe1f0,t3.small,subnet-xxxxxxxxxxxxxxxxx
</file>

<file path="template.yaml">
AWSTemplateFormatVersion: '2010-09-09'
Transform: AWS::Serverless-2016-10-31
Description: >-
  Stack to automatically create EC2 instances from CSV files uploaded to S3.
  Creates VPC, Subnet, S3 bucket, and Lambda function.

Resources:
  # ------------------------------------------------------------
  #  Network Resources
  # ------------------------------------------------------------
  VPC:
    Type: AWS::EC2::VPC
    Properties:
      CidrBlock: 10.0.0.0/16
      EnableDnsSupport: true
      EnableDnsHostnames: true
      Tags:
        - Key: Name
          Value: !Sub "${AWS::StackName}-VPC"

  InternetGateway:
    Type: AWS::EC2::InternetGateway
    Properties:
      Tags:
        - Key: Name
          Value: !Sub "${AWS::StackName}-IGW"

  VPCGatewayAttachment:
    Type: AWS::EC2::VPCGatewayAttachment
    Properties:
      VpcId: !Ref VPC
      InternetGatewayId: !Ref InternetGateway

  PublicSubnet:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref VPC
      CidrBlock: 10.0.1.0/24
      MapPublicIpOnLaunch: true # Automatically assign public IP to EC2
      Tags:
        - Key: Name
          Value: !Sub "${AWS::StackName}-PublicSubnet"

  RouteTable:
    Type: AWS::EC2::RouteTable
    Properties:
      VpcId: !Ref VPC
      Tags:
        - Key: Name
          Value: !Sub "${AWS::StackName}-RouteTable"

  DefaultRoute:
    Type: AWS::EC2::Route
    DependsOn: VPCGatewayAttachment
    Properties:
      RouteTableId: !Ref RouteTable
      DestinationCidrBlock: 0.0.0.0/0
      GatewayId: !Ref InternetGateway

  SubnetRouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref PublicSubnet
      RouteTableId: !Ref RouteTable

  # ------------------------------------------------------------
  #  S3 Bucket
  # ------------------------------------------------------------
  CsvFileBucket:
    Type: AWS::S3::Bucket
    Properties:
      # Bucket name must be globally unique, so includes Account ID and Region
      BucketName: !Sub "csv-to-ec2-${AWS::AccountId}-${AWS::Region}"
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true

  # ------------------------------------------------------------
  #  Lambda Function & IAM Role
  # ------------------------------------------------------------
  CsvToEc2Function:
    Type: AWS::Serverless::Function
    Properties:
      FunctionName: !Sub "${AWS::StackName}-CsvToEc2Function"
      Handler: app.lambda_handler
      Runtime: python3.12
      CodeUri: src/
      Timeout: 60
      # IAM policies for this Lambda function
      Policies:
        # 1. Permission to read CSV files from S3 bucket
        - S3ReadPolicy:
            BucketName: !GetAtt CsvFileBucket.Name
        # 2. Permission to create and tag EC2 instances
        - Statement:
          - Effect: Allow
            Action:
              - ec2:RunInstances
              - ec2:CreateTags
            Resource: "*" # In production, specify resources more strictly
      # Lambda trigger configuration
      Events:
        S3CsvUpload:
          Type: S3
          Properties:
            Bucket: !Ref CsvFileBucket
            Events: s3:ObjectCreated:*
            Filter:
              S3Key:
                Rules:
                  - Name: suffix
                    Value: .csv

Outputs:
  S3BucketName:
    Description: "S3 bucket name for uploading CSV files to create EC2 instances"
    Value: !Ref CsvFileBucket
  VpcId:
    Description: "ID of the created VPC"
    Value: !Ref VPC
  PublicSubnetId:
    Description: "ID of the created public subnet. Please specify this value in the CSV file."
    Value: !Ref PublicSubnet
</file>

<file path="README.md">
# シンプルEC2自動作成システム

S3にアップロードされたCSVファイルに基づき、EC2インスタンスを自動で作成します。

## アーキテクチャ

![アーキテクチャ図](images/アーキテクチャ図.png)

## 使い方

1.  **デプロイ**

    下記のコマンドでAWSリソース（VPC, S3, Lambda等）をデプロイします。
    コマンド実行後、Outputとして表示される `S3BucketName` と `PublicSubnetId` の値を控えてください。

    ```bash
    sam build
    sam deploy --guided
    ```

2.  **CSVファイルの編集とアップロード**

    `sample.csv` を開き、`subnet_id` の列を、手順1で控えた `PublicSubnetId` の値に書き換えます。
    その後、編集したCSVファイルをデプロイ済みのS3バケットにアップロードします。

    ```bash
    # <S3BucketName> を実際の名前に置き換えてください
    aws s3 cp sample.csv s3://<S3BucketName>/
    ```

    アップロード後、自動でEC2インスタンスが作成されます。

3.  **クリーンアップ**

    作成したリソースをすべて削除するには、以下のコマンドを実行します。

    ```bash
    sam delete
</file>

</files>
