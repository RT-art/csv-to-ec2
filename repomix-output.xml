This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.gitignore
manage.sh
README.md
sample.csv
src/app.py
template.yaml
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".gitignore">
# SAM build artifacts

.aws-sam/

# Local configuration file

.sam_outputs
</file>

<file path="manage.sh">
#!/bin/bash

# csv-to-ec2 SAMアプリケーションのデプロイと管理を行うスクリプト

# エラー発生時にスクリプトを停止
set -e

# --- 設定 ---
# SAMデプロイのアウトプット（S3バケット名など）を保存するファイル
CONFIG_FILE=".sam_outputs"

# CloudFormationのスタック名（変更可能）
STACK_NAME="csv-to-ec2-stack"

# AWSリージョンを自動取得。設定がなければ東京リージョンをデフォルトに
REGION=$(aws configure get region)
[ -z "$REGION" ] && REGION="ap-northeast-1"

# --- ヘルパー関数 ---
# 使い方を表示
usage() {
    echo "使い方: $0 {deploy|upload|delete}"
    echo
    echo "コマンド:"
    echo "  deploy   : AWS SAMスタックをビルド・デプロイします。ネットワークリソース、S3バケット、Lambdaを作成します。"
    echo "  upload   : S3バケットにCSVファイルをアップロードし、EC2インスタンス作成をトリガーします。"
    echo "             引数なしの場合、カレントディレクトリに存在する単一の.csvファイルを自動で検出します。"
    echo "  delete   : 作成されたすべてのAWSリソース（SAMスタック）を削除します。"
    echo
}

# --- メイン関数 ---
# SAMスタックのビルドとデプロイ
deploy_stack() {
    echo ">>> SAMアプリケーションをビルドしています..."
    sam build

    echo ">>> スタック '$STACK_NAME' をリージョン '$REGION' にデプロイしています..."
    # --guided オプションは初回デプロイ時に役立ちます。2回目以降は samconfig.toml が使用されます。
    sam deploy --stack-name "$STACK_NAME" --region "$REGION" --capabilities CAPABILITY_IAM --resolve-s3 --confirm-changeset

    echo ">>> デプロイ成功。S3バケット名を取得しています..."

    # CloudFormationスタックのアウトプットからS3バケット名を取得
    BUCKET_NAME=$(aws cloudformation describe-stacks \
        --stack-name "$STACK_NAME" \
        --query "Stacks[0].Outputs[?OutputKey=='S3BucketName'].OutputValue" \
        --output text \
        --region "$REGION")

    if [ -z "$BUCKET_NAME" ]; then
        echo "エラー: S3バケット名を取得できませんでした。"
        echo "AWSマネジメントコンソールでスタックの状態を確認してください。"
        exit 1
    fi

    # 後続の 'upload' コマンドで使うため、バケット名をファイルに保存
    echo "S3_BUCKET_NAME=$BUCKET_NAME" > "$CONFIG_FILE"

    echo "✔ セットアップ完了。"
    echo "  S3バケット '$BUCKET_NAME' の準備ができました。"
    echo "  './manage.sh upload' コマンドでEC2インスタンスを作成できます。"
}

# S3バケットへCSVファイルをアップロード
upload_csv() {
    if [ ! -f "$CONFIG_FILE" ]; then
        echo "エラー: 設定ファイル '$CONFIG_FILE' が見つかりません。"
        echo "最初に './manage.sh deploy' を実行してください。"
        exit 1
    fi

    # 設定ファイルからバケット名を読み込む
    source "$CONFIG_FILE"

    if [ -z "$S3_BUCKET_NAME" ]; then
        echo "エラー: '$CONFIG_FILE' 内に S3_BUCKET_NAME が見つかりません。"
        exit 1
    fi

    CSV_FILE="$1" # 引数で渡されたファイルパス
    if [ -z "$CSV_FILE" ]; then
        # 引数がない場合、カレントディレクトリから.csvファイルを自動検出
        shopt -s nullglob
        CSV_FILES=(*.csv)
        shopt -u nullglob

        if [ ${#CSV_FILES[@]} -eq 1 ]; then
            CSV_FILE="${CSV_FILES[0]}"
            echo ">>> CSVファイルを自動検出しました: '$CSV_FILE'"
        elif [ ${#CSV_FILES[@]} -eq 0 ]; then
            echo "エラー: カレントディレクトリにCSVファイルが見つかりません。"
            echo "CSVファイルを配置するか、ファイルパスを指定してください:"
            echo "  使用例: ./manage.sh upload sample.csv"
            exit 1
        else
            echo "エラー: 複数のCSVファイルが見つかりました。アップロードするファイルを指定してください:"
            printf " - %s\n" "${CSV_FILES[@]}"
            echo "  使用例: ./manage.sh upload ${CSV_FILES[0]}"
            exit 1
        fi
    fi

    if [ ! -f "$CSV_FILE" ]; then
        echo "エラー: CSVファイル '$CSV_FILE' が見つかりません。"
        exit 1
    fi

    echo ">>> '$CSV_FILE' をバケット '$S3_BUCKET_NAME' にアップロードしています..."
    aws s3 cp "$CSV_FILE" "s3://$S3_BUCKET_NAME/"

    echo "✔ ファイルがアップロードされました。EC2インスタンスの作成がトリガーされます。"
}

# SAMスタックの削除
delete_stack() {
    echo ">>> SAMスタック '$STACK_NAME' をリージョン '$REGION' から削除します..."
    echo "!!! 注意: この操作により、本スクリプトで作成されたすべてのAWSリソースが削除されます。!!!"

    sam delete --stack-name "$STACK_NAME" --region "$REGION" --no-prompts

    # ローカルの設定ファイルをクリーンアップ
    if [ -f "$CONFIG_FILE" ]; then
        rm "$CONFIG_FILE"
    fi

    echo "✔ スタックの削除が完了しました。"
}

# --- スクリプトのエントリポイント ---
# 必要なコマンドの存在チェック
if ! command -v sam &> /dev/null || ! command -v aws &> /dev/null; then
    echo "エラー: 'sam' および 'aws' CLIが必要です。"
    echo "これらをインストールし、AWS認証情報を設定してください。"
    exit 1
fi

# メインのコマンド振り分け
case "$1" in
    deploy)
        deploy_stack
        ;;
    upload)
        # 2番目の引数（ファイル名）をupload関数に渡す
        upload_csv "$2"
        ;;
    delete)
        delete_stack
        ;;
    ""|--help|-h) # 引数なし、またはヘルプオプションの場合
        usage
        exit 0
        ;;
    *)
        echo "エラー: 不明なコマンド '$1'"
        usage
        exit 1
        ;;
esac

exit 0
</file>

<file path="src/app.py">
import boto3
import csv
import os
import urllib.parse

s3 = boto3.client('s3')
ec2 = boto3.client('ec2')

def lambda_handler(event, context):
    """
    S3へのCSVファイルアップロードをトリガーに実行されるメイン関数。
    CSVファイルの内容に基づき、EC2インスタンスを作成します。
    作成されるすべてのインスタンスには、SSM（Systems Manager）が有効化されます。
    """
    # イベント情報からバケット名とファイル名（キー）を取得
    bucket = event['Records'][0]['s3']['bucket']['name']
    # URLエンコードされたキーをデコード
    key = urllib.parse.unquote_plus(event['Records'][0]['s3']['object']['key'], encoding='utf-8')

    print(f"処理開始: s3://{bucket}/{key}")

    try:
        # 環境変数から必須設定を取得
        target_subnet_id = os.environ['TARGET_SUBNET_ID']
        ssm_instance_profile_arn = os.environ['SSM_INSTANCE_PROFILE_ARN']
    except KeyError as e:
        print(f"致命的なエラー: 環境変数 {e} が設定されていません。")
        raise

    try:
        # S3からCSVファイルを取得して読み込む
        response = s3.get_object(Bucket=bucket, Key=key)
        # splitlines()で各行をリストに
        lines = response['Body'].read().decode('utf-8').splitlines()
        reader = csv.DictReader(lines)

        for row in reader:
            try:
                ami_id = row.get('ami_id')
                instance_type = row.get('instance_type')

                # 必須項目がなければスキップ
                if not all([ami_id, instance_type]):
                    print(f"必須項目（ami_id, instance_type）が不足しているため、この行をスキップします: {row}")
                    continue
                
                print(f"EC2インスタンスを作成中... AMI: {ami_id}, タイプ: {instance_type}")

                # EC2インスタンス作成APIのパラメータを設定
                run_instances_params = {
                    'ImageId': ami_id,
                    'InstanceType': instance_type,
                    'SubnetId': target_subnet_id,
                    'MinCount': 1,
                    'MaxCount': 1,
                    # SSMを有効にするためのIAMインスタンスプロファイルを常にアタッチ
                    'IamInstanceProfile': {
                        'Arn': ssm_instance_profile_arn
                    },
                    'TagSpecifications': [{
                        'ResourceType': 'instance',
                        'Tags': [
                            {'Key': 'Name', 'Value': f'auto-created-from-{os.path.basename(key)}'},
                            {'Key': 'SourceFile', 'Value': f's3://{bucket}/{key}'}
                        ]
                    }]
                }
                
                instance_response = ec2.run_instances(**run_instances_params)
                
                instance_id = instance_response['Instances'][0]['InstanceId']
                print(f"インスタンス作成成功: {instance_id}")

            except Exception as e:
                # 行単位のエラーはログに出力して処理を続行
                print(f"行の処理中にエラーが発生したためスキップします: {row}, エラー: {e}")

    except Exception as e:
        print(f"致命的なエラーが発生しました: {e}")
        # 致命的なエラーの場合はLambdaの実行を失敗させる
        raise

    print(f"処理完了: s3://{bucket}/{key}")
    return {'status': 'success'}
</file>

<file path="sample.csv">
ami_id,instance_type
ami-0c55b159cbfafe1f0,t2.micro
ami-0c55b159cbfafe1f0,t3.small
</file>

<file path="template.yaml">
AWSTemplateFormatVersion: '2010-09-09'
Transform: AWS::Serverless-2016-10-31
Description: >-
  S3にアップロードされたCSVファイルからEC2インスタンスを自動作成するスタック。
  VPC、サブネット、S3バケット、Lambda関数、およびEC2インスタンス用のIAMロールを作成します。
  作成されるEC2インスタンスは、デフォルトでSSM（Systems Manager）が有効になります。

Resources:

  # ------------------------------------------------------------
  # ネットワークリソース
  # ------------------------------------------------------------
  VPC:
    Type: AWS::EC2::VPC
    Properties:
      CidrBlock: 10.0.0.0/16
      EnableDnsSupport: true
      EnableDnsHostnames: true
      Tags:
        - Key: Name
          Value: !Sub "${AWS::StackName}-VPC"

  InternetGateway:
    Type: AWS::EC2::InternetGateway
    Properties:
      Tags:
        - Key: Name
          Value: !Sub "${AWS::StackName}-IGW"

  VPCGatewayAttachment:
    Type: AWS::EC2::VPCGatewayAttachment
    Properties:
      VpcId: !Ref VPC
      InternetGatewayId: !Ref InternetGateway

  PublicSubnet:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref VPC
      CidrBlock: 10.0.1.0/24
      MapPublicIpOnLaunch: true # パブリックIPを自動割り当て
      Tags:
        - Key: Name
          Value: !Sub "${AWS::StackName}-PublicSubnet"

  RouteTable:
    Type: AWS::EC2::RouteTable
    Properties:
      VpcId: !Ref VPC
      Tags:
        - Key: Name
          Value: !Sub "${AWS::StackName}-RouteTable"

  DefaultRoute:
    Type: AWS::EC2::Route
    DependsOn: VPCGatewayAttachment
    Properties:
      RouteTableId: !Ref RouteTable
      DestinationCidrBlock: 0.0.0.0/0
      GatewayId: !Ref InternetGateway

  SubnetRouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref PublicSubnet
      RouteTableId: !Ref RouteTable

  # ------------------------------------------------------------
  # EC2インスタンス用IAMロール（SSM有効化のため）
  # ------------------------------------------------------------
  EC2InstanceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service: "ec2.amazonaws.com"
            Action: "sts:AssumeRole"
      # Systems Managerを利用するためのAWS管理ポリシー
      ManagedPolicyArns:
        - "arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore"
      Tags:
        - Key: Name
          Value: !Sub "${AWS::StackName}-EC2InstanceRole"

  EC2InstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Roles:
        - !Ref EC2InstanceRole

  # ------------------------------------------------------------
  # S3バケット（CSVファイルアップロード用）
  # ------------------------------------------------------------
  CsvFileBucket:
    Type: AWS::S3::Bucket
    Properties:
      # バケット名はグローバルで一意にするため、アカウントIDとリージョンを含める
      BucketName: !Sub "csv-to-ec2-${AWS::AccountId}-${AWS::Region}"
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true

  # ------------------------------------------------------------
  # Lambda関数 & 実行ロール
  # ------------------------------------------------------------
  CsvToEc2Function:
    Type: AWS::Serverless::Function
    Properties:
      FunctionName: !Sub "${AWS::StackName}-CsvToEc2Function"
      Handler: app.lambda_handler
      Runtime: python3.12 # LTSであるPython 3.12を使用
      CodeUri: src/
      Timeout: 60
      Environment:
        Variables:
          TARGET_SUBNET_ID: !Ref PublicSubnet
          SSM_INSTANCE_PROFILE_ARN: !GetAtt EC2InstanceProfile.Arn
      Policies:
        # 1. S3バケットからCSVファイルを読み取る権限
        - S3ReadPolicy:
            BucketName: !GetAtt CsvFileBucket.Name
        # 2. EC2インスタンスを作成、タグ付けし、IAMロールをアタッチする権限
        - Statement:
            - Effect: Allow
              Action:
                - ec2:RunInstances
                - ec2:CreateTags
              Resource: "*" # 本番環境では、作成するリソースに応じてより厳密な権限設定を推奨
            - Effect: Allow
              Action: iam:PassRole
              Resource: !GetAtt EC2InstanceRole.Arn # 特定のIAMロールのみをEC2に渡すことを許可
      # Lambdaのトリガー設定 (S3へのCSVファイル作成)
      Events:
        S3CsvUpload:
          Type: S3
          Properties:
            Bucket: !Ref CsvFileBucket
            Events: s3:ObjectCreated:*
            Filter:
              S3Key:
                Rules:
                  - Name: suffix
                    Value: .csv

Outputs:
  S3BucketName:
    Description: "CSVファイルをアップロードするS3バケット名"
    Value: !Ref CsvFileBucket
  VpcId:
    Description: "作成されたVPCのID"
    Value: !Ref VPC
  PublicSubnetId:
    Description: "作成されたパブリックサブネットのID"
    Value: !Ref PublicSubnet
</file>

<file path="README.md">
# サーバーレスEC2プロビジョニング from CSV ✨

S3にアップロードされたCSVファイルに基づき、EC2インスタンスを自動で作成するAWS SAMアプリケーションです。

管理スクリプト (`manage.sh`) を利用することで、AWSリソースのデプロイからインスタンス作成のトリガーとなるファイルアップロードまで、簡単なコマンドで実行できます。作成されるすべてのEC2インスタンスには、**デフォルトでSSM（AWS Systems Manager）が有効化**されます。

## 🏛️ アーキテクチャ

![アーキテクチャ図](images/アーキテクチャ図.png)

1.  ユーザーが `manage.sh upload` コマンドでCSVファイルをS3バケットにアップロードします。
2.  S3へのファイルアップロードをトリガーとして、Lambda関数が実行されます。
3.  Lambda関数はCSVファイルの内容を読み取ります。
4.  CSVの各行で指定されたAMI IDとインスタンスタイプに基づき、EC2インスタンスを作成します。
5.  作成されるすべてのインスタンスには、SSM接続を許可するIAMロールがアタッチされます。

## 🚀 使い方

**前提条件:**

  - [AWS CLI](https://aws.amazon.com/cli/) と [AWS SAM CLI](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-cli-install.html) がインストール済みであること。
  - AWS認証情報が設定済みであること (`aws configure`)。

-----

### **Step 1: デプロイ (初回のみ)**

最初に、AWS環境に必要なリソース一式（VPC, S3バケット, Lambda関数など）をデプロイします。この操作は一度だけ行います。

1.  スクリプトに実行権限を付与します。

    ```bash
    chmod +x manage.sh
    ```

2.  デプロイコマンドを実行します。

    ```bash
    ./manage.sh deploy
    ```

    CloudFormationスタックの作成が完了すれば、環境の準備は完了です。

-----

### **Step 2: EC2インスタンスの作成**

1.  **CSVファイルを用意します。**
    `sample.csv` を参考に、作成したいEC2インスタンスの情報を記述します。

      - **`ami_id`** と **`instance_type`** の列は**必須**です。

    **`sample.csv` の例:**

    ```csv
    ami_id,instance_type
    ami-0abcdef1234567890,t2.micro
    ami-0abcdef1234567890,t3.small
    ```

2.  **CSVファイルをアップロードしてインスタンスを作成します。**
    `upload` コマンドを実行すると、Lambda関数がトリガーされ、CSVの内容に基づいてEC2インスタンスが自動で作成されます。

      - **【推奨】単一のCSVファイルを自動検出**
        カレントディレクトリにCSVファイルが1つだけ存在する場合、ファイル名を指定しなくても自動で検知してアップロードします。

        ```bash
        # カレントディレクトリにある唯一の.csvファイルをアップロード
        ./manage.sh upload
        ```

      - **特定のファイルを指定**
        複数のCSVファイルがある場合や、特定のファイルを指定したい場合は、ファイル名を引数として渡します。

        ```bash
        # my_instances.csv を指定してアップロード
        ./manage.csv upload my_instances.csv
        ```

-----

### **Step 3: クリーンアップ 🗑️**

このアプリケーションで作成したすべてのAWSリソースを削除するには、以下のコマンドを実行します。

```bash
./manage.sh delete
```

VPC、S3バケット、Lambda関数、IAMロールなど、関連リソースが一括で削除されます。
</file>

</files>
