This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.gitignore
manage.sh
README.md
sample.csv
src/app.py
template.yaml
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".gitignore">
# SAM build artifacts

.aws-sam/

# Local configuration file

.sam_outputs
</file>

<file path="manage.sh">
#!/bin/bash

# A script to manage the deployment and usage of the csv-to-ec2 SAM application.

# Stop script on any error
set -e

# --- Configuration ---
# File to store the outputs from the SAM deployment, like the S3 bucket name.
CONFIG_FILE=".sam_outputs"
DEFAULT_CSV="sample.csv"
STACK_NAME="csv-to-ec2-stack" # You can change this default stack name
REGION=$(aws configure get region)
[ -z "$REGION" ] && REGION="ap-northeast-1" # Default to Tokyo if not set

# --- Helper Functions ---
# Print usage instructions
usage() {
    echo "Usage: $0 {deploy|upload|delete}"
    echo
    echo "Commands:"
    echo "  deploy   : Builds and deploys the AWS SAM stack. Creates network resources, S3 bucket, and Lambda."
    echo "  upload   : Uploads a CSV file to the S3 bucket to trigger instance creation. Uses '$DEFAULT_CSV' by default."
    echo "  delete   : Deletes the entire AWS SAM stack and all created resources."
    echo
}

# --- Main Functions ---

# Build and Deploy the SAM stack
deploy_stack() {
    echo ">>> Building the SAM application..."
    sam build

    echo ">>> Deploying the stack '$STACK_NAME' to region '$REGION'..."
    # Using --guided for the first time is good, but subsequent deploys can use samconfig.toml
    sam deploy --stack-name "$STACK_NAME" --region "$REGION" --capabilities CAPABILITY_IAM --resolve-s3 --confirm-changeset

    echo ">>> Deployment successful. Fetching S3 bucket name..."
    
    # Get the S3 bucket name from the CloudFormation stack outputs
    BUCKET_NAME=$(aws cloudformation describe-stacks \
        --stack-name "$STACK_NAME" \
        --query "Stacks[0].Outputs[?OutputKey=='S3BucketName'].OutputValue" \
        --output text \
        --region "$REGION")

    if [ -z "$BUCKET_NAME" ]; then
        echo "Error: Could not retrieve S3 bucket name from stack outputs."
        echo "Please check the AWS Management Console for the stack status."
        exit 1
    fi

    # Save the bucket name to the config file for the 'upload' command
    echo "S3_BUCKET_NAME=$BUCKET_NAME" > "$CONFIG_FILE"
    
    echo "âœ” Setup complete."
    echo "  S3 Bucket '$BUCKET_NAME' is ready."
    echo "  You can now use './manage.sh upload' to create EC2 instances."
}

# Upload a CSV file to the S3 bucket
# Upload a CSV file to the S3 bucket
upload_csv() {
    if [ ! -f "$CONFIG_FILE" ]; then
        echo "Error: Configuration file '$CONFIG_FILE' not found."
        echo "Please run './manage.sh deploy' first to set up the environment."
        exit 1
    fi

    # Load the bucket name from the config file
    source "$CONFIG_FILE"

    if [ -z "$S3_BUCKET_NAME" ]; then
        echo "Error: S3_BUCKET_NAME not found in '$CONFIG_FILE'."
        exit 1
    fi

    # --- vvv ã“ã“ã‹ã‚‰å¤‰æ›´ vvv ---

    # Determine the CSV file to upload.
    # If an argument is given, use it. Otherwise, find a unique .csv file.
    if [ -n "$1" ]; then
        CSV_FILE="$1"
    else
        # Find .csv files in the current directory (excluding subdirectories)
        CSV_FILES=$(find . -maxdepth 1 -type f -name "*.csv")
        # Count the number of found files (trims whitespace for accuracy)
        NUM_CSV_FILES=$(echo "$CSV_FILES" | sed '/^$/d' | wc -l | tr -d ' ')

        if [ "$NUM_CSV_FILES" -eq 1 ]; then
            CSV_FILE=$(echo "$CSV_FILES")
            echo ">>> Auto-detected CSV file: '$CSV_FILE'"
        elif [ "$NUM_CSV_FILES" -eq 0 ]; then
            echo "Error: No CSV file found in the current directory."
            echo "Please place a single .csv file here or specify the file path:"
            echo "  ./manage.sh upload <your-file.csv>"
            exit 1
        else
            echo "Error: Multiple CSV files found. Please specify which one to upload:"
            echo "$CSV_FILES"
            echo "Usage: ./manage.sh upload <file-to-upload.csv>"
            exit 1
        fi
    fi

    # --- ^^^ ã“ã“ã¾ã§å¤‰æ›´ ^^^ ---

    if [ ! -f "$CSV_FILE" ]; then
        echo "Error: CSV file '$CSV_FILE' not found."
        exit 1
    fi

    echo ">>> Uploading '$CSV_FILE' to bucket '$S3_BUCKET_NAME'..."
    aws s3 cp "$CSV_FILE" "s3://$S3_BUCKET_NAME/"

    echo "âœ” File uploaded successfully. EC2 instance creation has been triggered."
}

# Delete the SAM stack
delete_stack() {
    echo ">>> Deleting the SAM stack '$STACK_NAME' from region '$REGION'..."
    echo "!!! This will remove all AWS resources created by this script. !!!"
    
    sam delete --stack-name "$STACK_NAME" --region "$REGION" --no-prompts

    # Clean up local config file
    if [ -f "$CONFIG_FILE" ]; then
        rm "$CONFIG_FILE"
    fi

    echo "âœ” Stack deleted successfully."
}


# --- Script Entrypoint ---
# Check for required tools
if ! command -v sam &> /dev/null || ! command -v aws &> /dev/null; then
    echo "Error: 'sam' and 'aws' CLI tools are required."
    echo "Please install them and configure your AWS credentials."
    exit 1
fi


# Main command router
case "$1" in
    deploy)
        deploy_stack
        ;;
    upload)
        # Pass the second argument (the filename) to the upload function
        upload_csv "$2"
        ;;
    delete)
        delete_stack
        ;;
    *)
        usage
        exit 1
        ;;
esac

exit 0
</file>

<file path="src/app.py">
import boto3
import csv
import os
import urllib.parse

s3 = boto3.client('s3')
ec2 = boto3.client('ec2')

SSM_INSTANCE_PROFILE_ARN = os.environ.get('SSM_INSTANCE_PROFILE_ARN')

def lambda_handler(event, context):
    """
    S3ã¸ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’ãƒˆãƒªã‚¬ãƒ¼ã«å®Ÿè¡Œã•ã‚Œã‚‹ãƒ¡ã‚¤ãƒ³é–¢æ•°ã€‚
    """
    # ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã‹ã‚‰ãƒã‚±ãƒƒãƒˆåã¨ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆã‚­ãƒ¼ï¼‰ã‚’å–å¾—
    bucket = event['Records'][0]['s3']['bucket']['name']
    key = urllib.parse.unquote_plus(event['Records'][0]['s3']['object']['key'], encoding='utf-8')
    
    print(f"å‡¦ç†é–‹å§‹: s3://{bucket}/{key}")

    try:
        subnet_id = os.environ['TARGET_SUBNET_ID']
    except KeyError:
        print("Fatal error: TARGET_SUBNET_ID environment variable is not set.")
        raise

    try:
        response = s3.get_object(Bucket=bucket, Key=key)
        lines = response['Body'].read().decode('utf-8').splitlines()
        reader = csv.DictReader(lines)

        for row in reader:
            try:
                ami_id = row.get('ami_id')
                instance_type = row.get('instance_type')

                if not all([ami_id, instance_type]):
                    print(f"Skipping row due to missing required fields: {row}")
                    continue

                run_instances_params = {
                    'ImageId': ami_id,
                    'InstanceType': instance_type,
                    'SubnetId': subnet_id,
                    'MinCount': 1,
                    'MaxCount': 1,
                    'TagSpecifications': [{
                        'ResourceType': 'instance',
                        'Tags': [
                            {'Key': 'Name', 'Value': f'auto-created-from-{os.path.basename(key)}'},
                            {'Key': 'SourceFile', 'Value': f's3://{bucket}/{key}'}
                        ]
                    }]
                }

                enable_ssm = row.get('enable_ssm', 'OFF').upper() == 'ON'
                
                if enable_ssm and SSM_INSTANCE_PROFILE_ARN:
                    run_instances_params['IamInstanceProfile'] = {
                        'Arn': SSM_INSTANCE_PROFILE_ARN
                    }
                    print(f"Creating EC2 instance with SSM enabled: AMI={ami_id}, Type={instance_type}")
                else:
                    print(f"Creating EC2 instance: AMI={ami_id}, Type={instance_type}")
                
                instance_response = ec2.run_instances(**run_instances_params)
                
                instance_id = instance_response['Instances'][0]['InstanceId']
                print(f"Instance creation successful: {instance_id}")

            except Exception as e:
                print(f"Error processing row, skipping: {row}, Error: {e}")

    except Exception as e:
        print(f"Fatal error occurred: {e}")
        raise e

    print(f"å‡¦ç†å®Œäº†: s3://{bucket}/{key}")
    return {'status': 'success'}
</file>

<file path="sample.csv">
ami_id,instance_type,enable_ssm
ami-0c55b159cbfafe1f0,t2.micro,ON
ami-0c55b159cbfafe1f0,t3.small,OFF
ami-0c55b159cbfafe1f0,t2.nano,
</file>

<file path="template.yaml">
AWSTemplateFormatVersion: '2010-09-09'
Transform: AWS::Serverless-2016-10-31
Description: >-
  Stack to automatically create EC2 instances from CSV files uploaded to S3.
  Creates VPC, Subnet, S3 bucket, and Lambda function.

Resources:
  # ------------------------------------------------------------
  #  Network Resources
  # ------------------------------------------------------------
  VPC:
    Type: AWS::EC2::VPC
    Properties:
      CidrBlock: 10.0.0.0/16
      EnableDnsSupport: true
      EnableDnsHostnames: true
      Tags:
        - Key: Name
          Value: !Sub "${AWS::StackName}-VPC"

  InternetGateway:
    Type: AWS::EC2::InternetGateway
    Properties:
      Tags:
        - Key: Name
          Value: !Sub "${AWS::StackName}-IGW"

  VPCGatewayAttachment:
    Type: AWS::EC2::VPCGatewayAttachment
    Properties:
      VpcId: !Ref VPC
      InternetGatewayId: !Ref InternetGateway

  PublicSubnet:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref VPC
      CidrBlock: 10.0.1.0/24
      MapPublicIpOnLaunch: true 
      Tags:
        - Key: Name
          Value: !Sub "${AWS::StackName}-PublicSubnet"

  RouteTable:
    Type: AWS::EC2::RouteTable
    Properties:
      VpcId: !Ref VPC
      Tags:
        - Key: Name
          Value: !Sub "${AWS::StackName}-RouteTable"

  DefaultRoute:
    Type: AWS::EC2::Route
    DependsOn: VPCGatewayAttachment
    Properties:
      RouteTableId: !Ref RouteTable
      DestinationCidrBlock: 0.0.0.0/0
      GatewayId: !Ref InternetGateway

  SubnetRouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref PublicSubnet
      RouteTableId: !Ref RouteTable

  # ------------------------------------------------------------
  #  IAM Role for EC2
  # ------------------------------------------------------------
  EC2InstanceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service: "ec2.amazonaws.com"
            Action: "sts:AssumeRole"
      ManagedPolicyArns:
        - "arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore"
      Tags:
        - Key: Name
          Value: !Sub "${AWS::StackName}-EC2InstanceRole"

  EC2InstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Roles:
        - !Ref EC2InstanceRole

  # ------------------------------------------------------------
  #  S3 Bucket
  # ------------------------------------------------------------
  CsvFileBucket:
    Type: AWS::S3::Bucket
    Properties:
      # Bucket name must be globally unique, so includes Account ID and Region
      BucketName: !Sub "csv-to-ec2-${AWS::AccountId}-${AWS::Region}"
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true

  # ------------------------------------------------------------
  #  Lambda Function & IAM Role
  # ------------------------------------------------------------
  CsvToEc2Function:
    Type: AWS::Serverless::Function
    Properties:
      FunctionName: !Sub "${AWS::StackName}-CsvToEc2Function"
      Handler: app.lambda_handler
      Runtime: python3.13
      CodeUri: src/
      Timeout: 60
      Environment:
        Variables:
          TARGET_SUBNET_ID: !Ref PublicSubnet
          SSM_INSTANCE_PROFILE_ARN: !GetAtt EC2InstanceProfile.Arn
      Policies:
        # 1. Permission to read CSV files from S3 bucket
        - S3ReadPolicy:
            BucketName: !GetAtt CsvFileBucket.Name
        # 2. Permission to create and tag EC2 instances
        - Statement:
          - Effect: Allow
            Action:
              - ec2:RunInstances
              - ec2:CreateTags
            Resource: "*" # In production, specify resources more strictly
          - Effect: Allow
            Action: iam:PassRole
            Resource: !GetAtt EC2InstanceRole.Arn
      # Lambda trigger configuration
      Events:
        S3CsvUpload:
          Type: S3
          Properties:
            Bucket: !Ref CsvFileBucket
            Events: s3:ObjectCreated:*
            Filter:
              S3Key:
                Rules:
                  - Name: suffix
                    Value: .csv

Outputs:
  S3BucketName:
    Value: !Ref CsvFileBucket
  VpcId:
    Value: !Ref VPC
  PublicSubnetId:
    Value: !Ref PublicSubnet
</file>

<file path="README.md">
# ã‚µãƒ¼ãƒãƒ¼ãƒ¬ã‚¹EC2ãƒ—ãƒ­ãƒ“ã‚¸ãƒ§ãƒ‹ãƒ³ã‚° from CSV âœ¨

CSVãƒ•ã‚¡ã‚¤ãƒ«ã«åŸºã¥ãã€EC2ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’è‡ªå‹•ã§ä½œæˆã™ã‚‹AWS SAMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã™ã€‚

## ğŸ›ï¸ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

![ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å›³](images/ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å›³.png)

-----

## ğŸš€ ä½¿ã„æ–¹

### **å‰ææ¡ä»¶**

  * [AWS CLI](https://aws.amazon.com/cli/) ã¨ [AWS SAM CLI](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-cli-install.html) ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã€‚
  * AWSèªè¨¼æƒ…å ±ãŒè¨­å®šæ¸ˆã¿ã§ã‚ã‚‹ã“ã¨ (`aws configure`)ã€‚

-----

### **Step 1: ãƒ‡ãƒ—ãƒ­ã‚¤ (åˆå›ã®ã¿)**

æœ€åˆã«ã€AWSç’°å¢ƒã«å¿…è¦ãªãƒªã‚½ãƒ¼ã‚¹ä¸€å¼ï¼ˆVPC, S3ãƒã‚±ãƒƒãƒˆ, Lambdaé–¢æ•°ãªã©ï¼‰ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã—ã¾ã™ã€‚ã“ã®æ“ä½œã¯ä¸€åº¦ã ã‘è¡Œã„ã¾ã™ã€‚

1.  **ã‚¹ã‚¯ãƒªãƒ—ãƒˆã«å®Ÿè¡Œæ¨©é™ã‚’ä»˜ä¸ã—ã¾ã™ã€‚**

    ```bash
    chmod +x manage.sh
    ```

2.  **ãƒ‡ãƒ—ãƒ­ã‚¤ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚**

    ```bash
    ./manage.sh deploy
    ```

    CloudFormationã‚¹ã‚¿ãƒƒã‚¯ã®ä½œæˆãŒå®Œäº†ã™ã‚Œã°ã€ç’°å¢ƒã®æº–å‚™ã¯å®Œäº†ã§ã™ã€‚

-----

### **Step 2: EC2ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ä½œæˆ**

1.  **CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”¨æ„ã—ã¾ã™ã€‚**
    `sample.csv` ã‚’å‚è€ƒã«ã€ä½œæˆã—ãŸã„EC2ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®æƒ…å ±ã‚’è¨˜è¿°ã—ã¾ã™ã€‚

      * `ami_id` ã¨ `instance_type` ã®åˆ—ã¯**å¿…é ˆ**ã§ã™ã€‚
      * `enable_ssm` åˆ—ã¯ä»»æ„ã§ã™ã€‚`ON`ã¨æŒ‡å®šã™ã‚‹ã¨ã€ãã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§SSMã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ãŒåˆ©ç”¨å¯èƒ½ã«ãªã‚Šã¾ã™ã€‚

2.  **CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆã—ã¾ã™ã€‚**
    `upload` ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€Lambdaé–¢æ•°ãŒãƒˆãƒªã‚¬ãƒ¼ã•ã‚Œã€CSVã®å†…å®¹ã«åŸºã¥ã„ã¦EC2ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒè‡ªå‹•ã§ä½œæˆã•ã‚Œã¾ã™ã€‚

      * **ã€æ¨å¥¨ã€‘CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒ1ã¤ã ã‘ã®å ´åˆ**
        ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒ1ã¤ã ã‘å­˜åœ¨ã™ã‚‹å ´åˆã€ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æŒ‡å®šã—ãªãã¦ã‚‚è‡ªå‹•ã§æ¤œçŸ¥ã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

        ```bash
        # ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚ã‚‹å”¯ä¸€çš„ã®.csvãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        ./manage.sh upload
        ```

      * **ç‰¹å®šã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã™ã‚‹å ´åˆ**
        è¤‡æ•°ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆã‚„ã€ç‰¹å®šã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã—ãŸã„å ´åˆã¯ã€ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å¼•æ•°ã¨ã—ã¦æ¸¡ã—ã¾ã™ã€‚

        ```bash
        # my_instances.csv ã‚’æŒ‡å®šã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        ./manage.sh upload my_instances.csv
        ```

-----

### **Step 3: ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— ğŸ—‘ï¸**

ã“ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ä½œæˆã—ãŸã™ã¹ã¦ã®AWSãƒªã‚½ãƒ¼ã‚¹ã‚’å‰Šé™¤ã™ã‚‹ã«ã¯ã€ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

```bash
./manage.sh delete
```

VPCã€S3ãƒã‚±ãƒƒãƒˆã€Lambdaé–¢æ•°ã€IAMãƒ­ãƒ¼ãƒ«ãªã©ã€é–¢é€£ãƒªã‚½ãƒ¼ã‚¹ãŒä¸€æ‹¬ã§å‰Šé™¤ã•ã‚Œã¾ã™ã€‚
</file>

</files>
