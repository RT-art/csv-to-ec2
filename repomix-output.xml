This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.aws-sam/build.toml
.aws-sam/build/CsvToEc2Function/app.py
.aws-sam/build/template.yaml
README.md
sample.json
src/app.py
template.yaml
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".aws-sam/build/CsvToEc2Function/app.py">
# src/app.py
import boto3
import csv
import os
import urllib.parse

s3 = boto3.client('s3')
ec2 = boto3.client('ec2')

def lambda_handler(event, context):
    bucket = event['Records'][0]['s3']['bucket']['name']
    key = urllib.parse.unquote_plus(event['Records'][0]['s3']['object']['key'], encoding='utf-8')
    
    print(f"Processing file: s3://{bucket}/{key}")

    try:
        response = s3.get_object(Bucket=bucket, Key=key)
        lines = response['Body'].read().decode('utf-8').splitlines()
        reader = csv.DictReader(lines)

        for row in reader:
            try:
                subnet_id = row.get('subnet_id')
                ami_id = row.get('ami_id')
                instance_type = row.get('instance_type')

                if not all([subnet_id, ami_id, instance_type]):
                    print(f"Skipping row due to missing required fields: {row}")
                    continue
                
                print(f"Creating EC2 instance with AMI: {ami_id}, Type: {instance_type} in Subnet: {subnet_id}")

                instance_response = ec2.run_instances(
                    ImageId=ami_id,
                    InstanceType=instance_type,
                    SubnetId=subnet_id,
                    MinCount=1,
                    MaxCount=1,
                    TagSpecifications=[{
                        'ResourceType': 'instance',
                        'Tags': [
                            {'Key': 'Name', 'Value': f'Created from {os.path.basename(key)}'},
                            {'Key': 'SourceFile', 'Value': f's3://{bucket}/{key}'}
                        ]
                    }]
                )
                
                instance_id = instance_response['Instances'][0]['InstanceId']
                print(f"Successfully created instance: {instance_id}")

            except Exception as e:
                print(f"Error processing row: {row}. Error: {str(e)}")

    except Exception as e:
        print(f"Error getting file from S3. Error: {str(e)}")
        raise e
</file>

<file path="sample.json">
[
{
"ami_id": "ami-0c55b159cbfafe1f0",
"instance_type": "t2.micro",
"enable_ssm": "ON"
},
{
"ami_id": "ami-0c55b159cbfafe1f0",
"instance_type": "t3.small",
"enable_ssm": "OFF"
},
{
"ami_id": "ami-0c55b159cbfafe1f0",
"instance_type": "t2.nano"
}
]
</file>

<file path=".aws-sam/build.toml">
# This file is auto generated by SAM CLI build command

[function_build_definitions.588ab450-6ac9-447f-bce9-cc1a830f7cb5]
codeuri = "C:\\Work\\github\\csv-to-ec2\\src"
runtime = "python3.13"
architecture = "x86_64"
handler = "app.lambda_handler"
manifest_hash = ""
packagetype = "Zip"
functions = ["CsvToEc2Function"]

[layer_build_definitions]
</file>

<file path=".aws-sam/build/template.yaml">
AWSTemplateFormatVersion: '2010-09-09'
Transform: AWS::Serverless-2016-10-31
Description: A simple and refined stack to create an EC2 instance from a CSV file
  in S3 using Lambda.
Resources:
  S3Bucket:
    Type: AWS::S3::Bucket
    DependsOn: S3InvokePermission
    Properties:
      BucketName:
        Fn::Sub: csv-ec2-creator-${AWS::AccountId}-${AWS::Region}
      NotificationConfiguration:
        LambdaConfigurations:
        - Event: s3:ObjectCreated:*
          Function:
            Fn::GetAtt:
            - CsvToEc2Function
            - Arn
          Filter:
            S3Key:
              Rules:
              - Name: suffix
                Value: .csv
  CsvToEc2Function:
    Type: AWS::Serverless::Function
    Properties:
      FunctionName:
        Fn::Sub: ${AWS::StackName}-CsvToEc2Function
      Handler: app.lambda_handler
      Runtime: python3.13
      Timeout: 60
      CodeUri: CsvToEc2Function
      Policies:
      - Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Action: s3:GetObject
          Resource:
            Fn::Sub: arn:aws:s3:::csv-ec2-creator-${AWS::AccountId}-${AWS::Region}/*
        - Effect: Allow
          Action: ec2:RunInstances
          Resource:
          - Fn::Sub: arn:aws:ec2:${AWS::Region}:${AWS::AccountId}:instance/*
          - Fn::Sub: arn:aws:ec2:${AWS::Region}:${AWS::AccountId}:volume/*
          - Fn::Sub: arn:aws:ec2:${AWS::Region}:${AWS::AccountId}:network-interface/*
          - Fn::Sub: arn:aws:ec2:${AWS::Region}:${AWS::AccountId}:security-group/*
          - Fn::Sub: arn:aws:ec2:${AWS::Region}:${AWS::AccountId}:subnet/*
          - Fn::Sub: arn:aws:ec2:${AWS::Region}::image/*
        - Effect: Allow
          Action: ec2:CreateTags
          Resource:
            Fn::Sub: arn:aws:ec2:${AWS::Region}:${AWS::AccountId}:instance/*
          Condition:
            StringEquals:
              ec2:CreateAction: RunInstances
    Metadata:
      SamResourceId: CsvToEc2Function
  S3InvokePermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName:
        Fn::GetAtt:
        - CsvToEc2Function
        - Arn
      Action: lambda:InvokeFunction
      Principal: s3.amazonaws.com
      SourceArn:
        Fn::Sub: arn:aws:s3:::csv-ec2-creator-${AWS::AccountId}-${AWS::Region}
Outputs:
  S3BucketName:
    Description: Bucket to upload CSV files.
    Value:
      Ref: S3Bucket
</file>

<file path="src/app.py">
import boto3
import json
import os
import urllib.parse

s3 = boto3.client('s3')
ec2 = boto3.client('ec2')

SSM_INSTANCE_PROFILE_ARN = os.environ.get('SSM_INSTANCE_PROFILE_ARN')

def lambda_handler(event, context):
    """
    Main function executed when a JSON file is uploaded to S3.
    """
    # Get bucket name and file name (key) from event information
    bucket = event['Records'][0]['s3']['bucket']['name']
    key = urllib.parse.unquote_plus(event['Records'][0]['s3']['object']['key'], encoding='utf-8')

    print(f"Processing started: s3://{bucket}/{key}")

    try:
        subnet_id = os.environ['TARGET_SUBNET_ID']
    except KeyError:
        print("Fatal error: TARGET_SUBNET_ID environment variable is not set.")
        raise

    try:
        response = s3.get_object(Bucket=bucket, Key=key)
        content = response['Body'].read().decode('utf-8')
        instances_to_create = json.loads(content)

        # Assume JSON content is a list
        if not isinstance(instances_to_create, list):
            print(f"Error: JSON file content is not a list. File: s3://{bucket}/{key}")
            return {'status': 'failed', 'reason': 'Invalid JSON format'}

        for item in instances_to_create:
            try:
                ami_id = item.get('ami_id')
                instance_type = item.get('instance_type')

                if not all([ami_id, instance_type]):
                    print(f"Skipping item due to missing required fields: {item}")
                    continue

                run_instances_params = {
                    'ImageId': ami_id,
                    'InstanceType': instance_type,
                    'SubnetId': subnet_id,
                    'MinCount': 1,
                    'MaxCount': 1,
                    'TagSpecifications': [{
                        'ResourceType': 'instance',
                        'Tags': [
                            {'Key': 'Name', 'Value': f'auto-created-from-{os.path.basename(key)}'},
                            {'Key': 'SourceFile', 'Value': f's3://{bucket}/{key}'}
                        ]
                    }]
                }

                enable_ssm = str(item.get('enable_ssm', 'OFF')).upper() == 'ON'
                
                if enable_ssm and SSM_INSTANCE_PROFILE_ARN:
                    run_instances_params['IamInstanceProfile'] = {
                        'Arn': SSM_INSTANCE_PROFILE_ARN
                    }
                    print(f"Creating EC2 instance with SSM enabled: AMI={ami_id}, Type={instance_type}")
                else:
                    print(f"Creating EC2 instance: AMI={ami_id}, Type={instance_type}")
                
                instance_response = ec2.run_instances(**run_instances_params)
                
                instance_id = instance_response['Instances'][0]['InstanceId']
                print(f"Instance creation successful: {instance_id}")

            except Exception as e:
                print(f"Error processing item, skipping: {item}, Error: {e}")

    except json.JSONDecodeError as e:
        print(f"Fatal error: Failed to decode JSON from s3://{bucket}/{key}. Error: {e}")
        raise e
    except Exception as e:
        print(f"Fatal error occurred: {e}")
        raise e

    print(f"Processing complete: s3://{bucket}/{key}")
    return {'status': 'success'}
</file>

<file path="template.yaml">
AWSTemplateFormatVersion: '2010-09-09'
Transform: AWS::Serverless-2016-10-31
Description: >-
  Stack to automatically create EC2 instances from JSON files uploaded to S3.
  Creates VPC, Subnet, S3 bucket, and Lambda function.

Resources:
  # ------------------------------------------------------------
  #  Network Resources
  # ------------------------------------------------------------
  VPC:
    Type: AWS::EC2::VPC
    Properties:
      CidrBlock: 10.0.0.0/16
      EnableDnsSupport: true
      EnableDnsHostnames: true
      Tags:
        - Key: Name
          Value: !Sub "${AWS::StackName}-VPC"

  InternetGateway:
    Type: AWS::EC2::InternetGateway
    Properties:
      Tags:
        - Key: Name
          Value: !Sub "${AWS::StackName}-IGW"

  VPCGatewayAttachment:
    Type: AWS::EC2::VPCGatewayAttachment
    Properties:
      VpcId: !Ref VPC
      InternetGatewayId: !Ref InternetGateway

  PublicSubnet:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref VPC
      CidrBlock: 10.0.1.0/24
      MapPublicIpOnLaunch: true # Automatically assign public IP to EC2
      Tags:
        - Key: Name
          Value: !Sub "${AWS::StackName}-PublicSubnet"

  RouteTable:
    Type: AWS::EC2::RouteTable
    Properties:
      VpcId: !Ref VPC
      Tags:
        - Key: Name
          Value: !Sub "${AWS::StackName}-RouteTable"

  DefaultRoute:
    Type: AWS::EC2::Route
    DependsOn: VPCGatewayAttachment
    Properties:
      RouteTableId: !Ref RouteTable
      DestinationCidrBlock: 0.0.0.0/0
      GatewayId: !Ref InternetGateway

  SubnetRouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref PublicSubnet
      RouteTableId: !Ref RouteTable

  # ------------------------------------------------------------
  #  IAM Role for EC2
  # ------------------------------------------------------------
  EC2InstanceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service: "ec2.amazonaws.com"
            Action: "sts:AssumeRole"
      ManagedPolicyArns:
        - "arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore"
      Tags:
        - Key: Name
          Value: !Sub "${AWS::StackName}-EC2InstanceRole"

  EC2InstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Roles:
        - !Ref EC2InstanceRole

  # ------------------------------------------------------------
  #  S3 Bucket
  # ------------------------------------------------------------
  JsonFileBucket:
    Type: AWS::S3::Bucket
    Properties:
      # Bucket name must be globally unique, so includes Account ID and Region
      BucketName: !Sub "json-to-ec2-${AWS::AccountId}-${AWS::Region}"
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true

  # ------------------------------------------------------------
  #  Lambda Function & IAM Role
  # ------------------------------------------------------------
  JsonToEc2Function:
    Type: AWS::Serverless::Function
    Properties:
      FunctionName: !Sub "${AWS::StackName}-JsonToEc2Function"
      Handler: app.lambda_handler
      Runtime: python3.13
      CodeUri: src/
      Timeout: 60
      Environment:
        Variables:
          TARGET_SUBNET_ID: !Ref PublicSubnet
          SSM_INSTANCE_PROFILE_ARN: !GetAtt EC2InstanceProfile.Arn
      Policies:
        # 1. Permission to read JSON files from S3 bucket
        - S3ReadPolicy:
            BucketName: !GetAtt JsonFileBucket.Name
        # 2. Permission to create and tag EC2 instances
        - Statement:
          - Effect: Allow
            Action:
              - ec2:RunInstances
              - ec2:CreateTags
            Resource: "*" # In production, specify resources more strictly
          - Effect: Allow
            Action: iam:PassRole
            Resource: !GetAtt EC2InstanceRole.Arn
      # Lambda trigger configuration
      Events:
        S3JsonUpload:
          Type: S3
          Properties:
            Bucket: !Ref JsonFileBucket
            Events: s3:ObjectCreated:*
            Filter:
              S3Key:
                Rules:
                  - Name: suffix
                    Value: .json

Outputs:
  S3BucketName:
    Description: "S3 bucket name for uploading JSON files to create EC2 instances"
    Value: !Ref JsonFileBucket
  VpcId:
    Description: "ID of the created VPC"
    Value: !Ref VPC
  PublicSubnetId:
    Description: "ID of the created public subnet. This is automatically used by the Lambda function."
    Value: !Ref PublicSubnet
</file>

<file path="README.md">
# Jsonを編集するだけで、サーバレスでEC2が構築されるAWS SAMデモ

S3にアップロードされたJSONファイルに基づき、EC2インスタンスを自動で作成します。

## アーキテクチャ

![アーキテクチャ図](images/アーキテクチャ図.png)

## 特徴

- **設定不要**: `sam deploy`でデプロイすれば、必要なVPCやSubnetも自動で構築。Subnet IDなどを手動で設定する必要はありません。
- **簡単操作**: 作成したいEC2の情報をJSON形式で記述し、S3にアップロードするだけで自動でインスタンスが作成されます。`"enable_ssm": "ON"` を追加すると、SSMセッションマネージャーで接続可能なインスタンスを作成できます。

## 使い方

1.  **デプロイ**

    下記のコマンドでAWSリソースをデプロイします。
    コマンド実行後、Outputとして表示される `S3BucketName` の値を控えてください。

    ```bash
    sam build
    sam deploy --guided
    ```

2.  **JSONファイルのアップロード**

    `sample.json` を参考に、作成したいEC2の情報を記載したJSONファイルを用意します。（`ami_id`と`instance_type`のキーは必須です。`enable_ssm` は任意です。）
    用意したJSONファイルを、手順1で控えたS3バケットにアップロードします。

    ```bash
    # <S3BucketName> を実際の名前に置き換えてください
    aws s3 cp sample.json s3://<S3BucketName>/
    ```

    アップロード後、自動でEC2インスタンスが作成されます。

3.  **クリーンアップ**

    作成したリソースをすべて削除するには、以下のコマンドを実行します。

    ```bash
    sam delete
</file>

</files>
